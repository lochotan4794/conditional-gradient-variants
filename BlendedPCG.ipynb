{"cells":[{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1699362198798,"execution_millis":10,"deepnote_to_be_reexecuted":false,"cell_id":"39cfeb7d33b84240a8dbafcf8f85fa22","deepnote_cell_type":"code"},"source":"import numpy as np","block_group":"39cfeb7d33b84240a8dbafcf8f85fa22","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1699362198799,"execution_millis":4210,"deepnote_to_be_reexecuted":false,"cell_id":"6fa9a8d045e341c1ae154d377205c73a","deepnote_cell_type":"code"},"source":"!pip install tableprint","block_group":"c0a52746b59a486ea242fb52e058a59c","execution_count":null,"outputs":[{"name":"stdout","text":"Collecting tableprint\n  Downloading tableprint-0.9.1-py3-none-any.whl (6.8 kB)\nRequirement already satisfied: wcwidth in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from tableprint) (0.2.5)\nRequirement already satisfied: future in /shared-libs/python3.9/py/lib/python3.9/site-packages (from tableprint) (0.18.2)\nInstalling collected packages: tableprint\nSuccessfully installed tableprint-0.9.1\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1699362202972,"execution_millis":190,"deepnote_to_be_reexecuted":false,"cell_id":"885bd4a301d146998ae0bb675a84bbe7","deepnote_cell_type":"code"},"source":"# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Fri Dec 05 21:30:01 2014\n\n@author: spokutta\n\"\"\"\n#################################\n# file names/directories\n#################################\nimport os\nlogging_dir = os.getcwd()\n\n#################################\n# other parameters\n#################################\nautograd = True\n\n######################################\n# Weak Separation Oracle Parameters\n######################################\nnodeLimit = None\naccuracyComparison = 1e-12\nggEps = 1e-08  # accuracy measure in early termination for LP solver\n\n\n#################################\n# Oracle Cache information\n#################################\nuseCache = True\npreviousPoints = {}\n\n\n###########################################\n# Backtracking Line Search Parameters\n###########################################\nls_tau = 0.5\nls_eps = 0.01\n\n###############################\n# Algorithm Configuration\n###############################\nrun_config = {\n        'solution_only': True,\n        'verbosity': 'normal',\n        'dual_gap_acc': 1e-06,\n        'runningTimeLimit': None,\n        'use_LPSep_oracle': True,\n        'max_lsFW': 30,\n        'strict_dropSteps': True,\n        'max_stepsSub': 200,\n        'max_lsSub': 30,\n        'LPsolver_timelimit': 100,\n        'K': 1\n        }\n\n","block_group":"95fefff06cf24d338261359926b8763e","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1699362202973,"execution_millis":190,"deepnote_to_be_reexecuted":false,"cell_id":"5199254678344c8e9c6cfe27a63ebb39","deepnote_cell_type":"code"},"source":"\"\"\"Create objective function by name.\n\nCurrently, the name is the name of a class from the module with almost\nthe same name.  The module name is obtained from name by converting\nall capital letters to lower case and inserting a _ before them.\nExceptions are capital letters following a _, which are left alone.\nAll leading _ are stripped from the module name.\n\nExamples:\n\n>>> import numpy as np\n\n>>> f = ObjectiveFunctionFactory.create_function(\n...        'standard_parabola', np.arange(3))\n>>> float(round(f.evaluate(np.array([1, 0, -1])), 5))\n11.0\n\n>>> ObjectiveFunctionFactory.create_function(\n...     'square_norm_Ax_minus_b', 4, 3, 2)\n... # doctest: +ELLIPSIS\n<objective_functions.square_norm_Ax_minus_b.square_norm_Ax_minus_b object at ...>\n>>> ObjectiveFunctionFactory.create_function(\n...     'lasso_function', 5)\n... # doctest: +ELLIPSIS\n<objective_functions.lasso_function.lasso_function object at ...>\n>>> ObjectiveFunctionFactory.create_function(\n...     'sdp_matrixCompletion', size=5, density=1/3,\n...     rank_ratio=2/5)\n... # doctest: +ELLIPSIS\n<objective_functions.sdp_matrix_completion.sdp_matrixCompletion object at ...>\n>>> ObjectiveFunctionFactory.create_function(\n...     'MatrixCompletion', np.arange(6).reshape(3, 2), 0.1)\n... # doctest: +ELLIPSIS\n<objective_functions.matrix_completion.MatrixCompletion object at ...>\n\"\"\"\n# TODO: Test the created functions.  Unfortunately, most of them is random, so hard to check for specific values.\n\nimport importlib\nimport re\n\n\nclass ObjectiveFunctionFactory(object):\n\n    @staticmethod\n    def create_function(name, *args, **kwargs):\n        def module_name(match):\n            return '_' + match.group().lower()\n\n        module_name = re.sub(r'(?<!_)[A-Z]', module_name, name) \\\n                        .lstrip('_')\n        module = importlib.import_module('objective_functions.'\n                                         + module_name)\n        obj_function = getattr(module, name)\n        return obj_function(*args, **kwargs)\n","block_group":"332591b7a4884776afdc31cd69a87cc6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1699362202974,"execution_millis":192,"deepnote_to_be_reexecuted":false,"cell_id":"59e70b9f62684d3a9465c5f89ae5848c","deepnote_cell_type":"code"},"source":"import abc\n\n\nclass ObjFunction(abc.ABC):\n    \"\"\"Objective function with gradient.\n\n    >>> import numpy as np\n    >>> class TestFunction(ObjFunction):\n    ...\n    ...     def evaluate(self, value):\n    ...         return value[0] ** 2 - value[1] ** 3\n    ...\n    ...     def gradient(self, value):\n    ...         return np.array([2 * value[0], -3 * value[1] ** 2])\n    >>> t = TestFunction()\n    >>> t.evaluate(np.array([1, -1]))\n    2\n    >>> t.gradient(np.array([1, -1]))\n    array([ 2, -3])\n    \"\"\"\n    @abc.abstractmethod\n    def evaluate(self, value):\n        \"\"\"Return function value at point value.\"\"\"\n        return\n\n    @abc.abstractmethod\n    def gradient(self, value):\n        \"\"\"Return gradient at point value.\n\n        >>> import numpy as np\n        >>> class TestFunction(ObjFunction):\n        ...\n        ...     def evaluate(self, value):\n        ...         return np.sum(value * value)\n        ...\n        ...     def gradient(self, value):\n        ...         return super().gradient(value)\n        >>> np.around(\n        ...     TestFunction().gradient(np.arange(6).reshape(2, 3)),\n        ...     5)\n        array([[  0.,   2.,   4.],\n               [  6.,   8.,  10.]])\n\n\n        Implementation notes:\n\n        scipy.optimize.approx_fprime doesn't support functions with\n        multidimensional array argument:\n\n        >>> import numpy as np\n        >>> from scipy.optimize import approx_fprime\n        >>> def x_x_transpose(x):\n        ...     return np.sum(x * x)\n        >>> np.around(approx_fprime(np.arange(6),\n        ...                         x_x_transpose,\n        ...                         np.sqrt(np.finfo(float).eps)),\n        ...           5)\n        array([  0.,   2.,   4.,   6.,   8.,  10.])\n        >>> np.around(approx_fprime(np.arange(6).reshape(2, 3),\n        ...                         x_x_transpose,\n        ...                         np.sqrt(np.finfo(float).eps)),\n        ...           5) #doctest: +IGNORE_EXCEPTION_DETAIL\n        Traceback (most recent call last):\n          ...\n        ValueError: operands could not be broadcast together with shapes (2,3) (2,)\n\n        Correct output should be:\n\n        array([[  0.,   2.,   4.],\n               [  6.,   8.,  10.]])\n        \"\"\"\n        from scipy.optimize import approx_fprime\n        import numpy as np\n        shape = value.shape\n\n        def f(x):\n            return self.evaluate(x.reshape(shape))\n\n        gradient = approx_fprime(value.reshape(-1),\n                                 f,\n                                 np.sqrt(np.finfo(float) .eps))\n        return gradient.reshape(shape)\n\n    def check_gradient(self, value, accuracy=None):\n        \"\"\"Verify gradient of function.\n\n        Return True if gradient is correct, otherwise the difference\n        in two norm.\n\n        Parameters:\n\n        value: the point at which to verify gradient\n\n        accuracy: allowed error in gradient in two norm, due to\n                  numerical errors and gradient estimation.\n\n        Examples:\n\n        >>> import numpy as np\n        >>> def f(x):\n        ...     return np.sum(x ** 3)\n        ...\n        >>> def f_grad(x):\n        ...     return 3 * x ** 2\n        ...\n        >>> def f_grad_wrong_shape(x):\n        ...     # Flattening array is an error\n        ...     return f_grad(x).reshape(-1)\n        ...\n        >>> def f_wrong_grad(x):\n        ...     return 2 * x ** 2\n        ...\n        >>> test = ObjectiveFunction(f, f_grad)\n        >>> test_wrong_shape = ObjectiveFunction(f, f_grad_wrong_shape)\n        >>> test_wrong_grad = ObjectiveFunction(f, f_wrong_grad)\n\n        >>> test.check_gradient(np.random.randint(10, size=(2, 3)),\n        ...                     10 ** -5)\n        True\n        >>> test.check_gradient(np.array([[2, 0, 1], [-1, 0, -3]]))\n        True\n        >>> test_wrong_shape.check_gradient(np.array([[2, 1, 1],\n        ...                                           [-1, 0, -3]]))\n        Traceback (most recent call last):\n            ...\n        ValueError: Gradient shape is (6,) instead of (2, 3)\n\n\n        The float() conversion below is a workaround for type\n        np.float64 not providing the shortest repersentation.\n\n        >>> float(round(test_wrong_grad.check_gradient(\n        ...     np.array([[2, 0, 1], [-1, 0, -3]])), 5))\n        9.94987\n        \"\"\"\n        from scipy.optimize import check_grad, approx_fprime\n        import numpy as np\n        shape = value.shape\n\n        def f(x):\n            return self.evaluate(x.reshape(shape))\n\n        if accuracy is None:\n            estimated_gradient = approx_fprime(value.reshape(-1), f,\n                                    np.sqrt(np.finfo(float).eps))\n            accuracy = (np.linalg.norm(estimated_gradient)\n                        * np.cbrt(np.finfo(float).eps))\n\n        def grad(x):\n            gradient = self.gradient(x.reshape(shape))\n            if gradient.shape != shape:\n                raise ValueError('Gradient shape is %s instead of %s'\n                                 % (gradient.shape, shape))\n            return gradient.reshape(-1)\n\n        error = check_grad(f, grad, value.reshape(-1))\n        return True if error <= accuracy else error\n\n\nclass ObjectiveFunction(ObjFunction):\n    \"\"\"Explicitly given objective function.\n\n    Example:\n\n    >>> import numpy as np\n    >>> def f(x):\n    ...     return np.sum(x ** 3)\n    ...\n    >>> def f_grad(x):\n    ...     return 3 * x ** 2\n    ...\n    >>> test = ObjectiveFunction(f)\n    >>> test2 = ObjectiveFunction(f, f_grad)\n\n    >>> test.evaluate(np.array([2, 1]))\n    9\n    >>> test2.evaluate(np.array([2, 1]))\n    9\n\n    >>> np.around(test.gradient(np.array([2, 1])), 5)\n    array([ 12.,   3.])\n    >>> test2.gradient(np.array([2, 1]))\n    array([12,  3])\n\n    >>> test.evaluate(np.array([[2, 1], [-1, -3]]))\n    -19\n    >>> test2.evaluate(np.array([[2, 1], [-1, -3]]))\n    -19\n\n    >>> np.around(test.gradient(np.array([[2, 1], [-1, -3]])), 5)\n    array([[ 12.,   3.],\n           [  3.,  27.]])\n    >>> test2.gradient(np.array([[2, 1], [-1, -3]]))\n    array([[12,  3],\n           [ 3, 27]])\n    \"\"\"\n    def __init__(self, function, gradient=None):\n        \"\"\"Wrap a function into an instance of ``ObjFunction``.\n\n        Parameters:\n\n        function: The objective function as a callable.\n        gradient: The gradient of ``function`` as a callable.\n                  If omitted the gradient will be estimated.\n        \"\"\"\n        self.__function = function\n        if gradient is not None:\n            self.__gradient = gradient\n        else:\n            self.__gradient = super().gradient\n\n    def evaluate(self, value):\n        return self.__function(value)\n\n    def gradient(self, value):\n        return self.__gradient(value)\n","block_group":"55665c24692f4e07ad2de35cf68ce318","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1699362202975,"execution_millis":306,"deepnote_to_be_reexecuted":false,"cell_id":"9c16690528c446e1997a9a2135f6e75d","deepnote_cell_type":"code"},"source":"from math import inf\nimport os\nimport numpy as np\nimport datetime\n\n################################\n# line search functions\n################################\ndef ternary_ls(obj_fct, x, direction, accuracy):\n    gamma_ub = 1\n    gamma_lb = 0\n    # initialize\n    y = x + direction  # end point\n    endpoint_val = obj_fct.evaluate(y)\n    val_y = endpoint_val\n    val_x = obj_fct.evaluate(x)\n    i = 0\n    while abs(val_y - val_x) > accuracy:\n        zx = x + 1/float(3) * (y - x)\n        zy = x + 2/float(3) * (y - x)\n        value_zx = obj_fct.evaluate(zx)\n        value_zy = obj_fct.evaluate(zy)\n        if value_zx < value_zy:\n            y = zy\n            gamma_ub = gamma_lb + (gamma_ub-gamma_lb) * 2/3\n            val_y = value_zy  # update value y because position of y changed\n        else:\n            x = zx\n            gamma_lb = gamma_lb + (gamma_ub-gamma_lb) * 1/3\n            val_x = value_zx  # update value x because position of x changed\n        i += 1\n    return gamma_lb, i\n\n\ndef backtracking_ls_FW(objectiveFunction, x, grad, direction, steps):\n    step_size = 1\n    grad_direction = np.inner(grad, direction)\n\n    i = 0\n    # assert grad_direction <= 0, 'grad_direction is {}'.format(grad_direction)\n    if grad_direction == 0:\n        return 0, i\n\n    evalu_oldpint = objectiveFunction.evaluate(x)\n    evalu_newpoint = objectiveFunction.evaluate(x + step_size * direction)\n    while (evalu_newpoint - evalu_oldpint) > ls_eps * step_size * grad_direction:\n        if i > steps:\n            if evalu_oldpint - evalu_newpoint >= 0:\n                return step_size, i\n            else:\n                return 0, i\n        step_size *= ls_tau\n        evalu_newpoint = objectiveFunction.evaluate(x + step_size * direction)\n        i += 1\n    # assert (evalu_oldpint - evalu_newpoint >= 0)\n    return step_size, i\n\n# Linearly Convergent Frank-Wolfe with Backtracking Line-Search\ndef backtracking_ls_delta(objectiveFunction, x, grad, direction, steps, step_size_ub):\n    step_size = step_size_ub\n    grad_direction = np.inner(grad, direction)\n    # print(\"grad_direction {}\".format(grad_direction))\n\n    i = 0\n    assert grad_direction <= 0, 'grad_direction is {}'.format(grad_direction)\n    if grad_direction == 0:\n        return 0, i\n\n    evalu_oldpint = objectiveFunction.evaluate(x)\n    evalu_newpoint = objectiveFunction.evaluate(x + step_size * direction)\n    while (evalu_newpoint - evalu_oldpint) > ls_eps * step_size * grad_direction:\n        if i > steps:\n            if evalu_oldpint - evalu_newpoint >= 0:\n                return step_size, i\n            else:\n                return 0, i\n        step_size *= ls_tau\n        evalu_newpoint = objectiveFunction.evaluate(x + step_size * direction)\n        i += 1\n    # assert (evalu_oldpint - evalu_newpoint >= 0)\n    return step_size, i\n\n\ndef backtracking_ls_on_alpha(alpha_list, objectiveFunction, s_list, step_size_ub, direction, steps,\n                             func_val_improve_last, strict_dropSteps = True):\n    \"\"\"\n    backtracking line search method from https://people.maths.ox.ac.uk/hauser/hauser_lecture2.pdf\n    used on sub-algorithm\n    \"\"\"\n\n    step_size = step_size_ub\n    grad_direction = -np.inner(direction, direction)\n\n    x_old = np.dot(np.transpose(s_list), alpha_list)\n    x_new = np.dot(np.transpose(s_list), alpha_list + step_size * direction)  # end point\n    evalu_oldpint = objectiveFunction.evaluate(x_old)\n    evalu_newpoint = objectiveFunction.evaluate(x_new)\n\n    # relax dropping criterion\n    if func_val_improve_last != 'N/A':\n        if not strict_dropSteps:\n            drop_criteria = min(0.5 * func_val_improve_last, ls_eps)\n        else:\n            drop_criteria = 0\n        if evalu_newpoint <= evalu_oldpint + drop_criteria:\n            return step_size, 0, 'P'\n\n    # begin line search\n    i = 0\n    while (evalu_newpoint - evalu_oldpint) > ls_eps*step_size * grad_direction:\n        if i > steps and evalu_newpoint - evalu_oldpint >= 0:\n            return 0, i, 'PS'\n        step_size *= ls_tau\n        x_new = np.dot(np.transpose(s_list), alpha_list + step_size * direction)\n        evalu_newpoint = objectiveFunction.evaluate(x_new)\n        i += 1\n    if evalu_newpoint >= evalu_oldpint:\n        return 0, i, 'PS'\n    return step_size, i, 'P'\n\n\n################################\n# cache functions:\n################################\ndef inSequence(array, sequence):\n    \"\"\"Return True when Numpy array is an element of sequence.\n\n    >>> inSequence(np.array([1,2,3]), [np.array([0,1,2]),\n    ...                                np.array([1.0, 2.0, 3.0])])\n    True\n\n    >>> inSequence(np.array([1,2,3]), [np.array([0,1,2]),\n    ...                                np.array([-2.0, 1.0, 3.0])])\n    False\n    \"\"\"\n    for i in sequence:\n        if np.all(array == i):\n            return True\n    return False\n\n\ndef removeFromCache(x):\n    \"\"\"Remove point x from cache if there.\n    >>> _ignore = reset_cache()\n    >>> for i in range(3):\n    ...     _ignore = addToCache(np.array([i]))\n    >>> removeFromCache(np.array([2]))\n    point deleted from cache, current number of points in cache 2\n    >>> removeFromCache(np.array([3]))\n\n    >>> removeFromCache(np.array([1]))\n    point deleted from cache, current number of points in cache 1\n    \"\"\"\n    global previousPoints\n    current_cache_length = len(previousPoints)\n    key = hash(x.tostring())\n    try:\n        del previousPoints[key]\n    except KeyError:\n        pass\n    else:\n        assert current_cache_length - len(previousPoints) == 1\n\n\ndef addToCache(x, clean=None):\n    global previousPoints\n    if clean:\n        result = dict(previousPoints)\n        current_value = np.inner(x, x)\n        for key, y in previousPoints.items():\n            if np.inner(x, y) > current_value:\n                result.pop(key)\n        previousPoints = result\n    key = hash(x.tostring())\n    if key not in previousPoints:\n        previousPoints[key] = x\n\n\ndef checkForCache(c, goal):\n    \"\"\"Search for a cached numpy array with small objective value.\n\n    c: objective\n    goal: upper bound on the acceptable objective value.\n\n    >>> reset_cache()\n\n    >>> _ignore = addToCache(np.array([1., 0.]))\n\n    >>> _ignore = addToCache(np.array([0., 1.]))\n\n    >>> checkForCache(np.array([1,2]), goal=1)\n    array([ 1.,  0.])\n\n    >>> checkForCache(np.array([2,1]), goal=1)\n    array([ 0.,  1.])\n\n    >>> checkForCache(np.array([1,3]), goal=.5)\n\n    \"\"\"\n    global previousPoints\n    for x in previousPoints.values():\n        if np.inner(c, x) <= goal:\n            break\n    else:\n        x = None\n    return x\n\n\ndef checkForPairwiseCache(c, c_tilde, goal):\n    mi = inf\n    x_plus = None\n    mi_tilde = inf\n    x_minus = None\n    global previousPoints\n    for x in previousPoints.values():\n        if np.inner(c, x) < mi:\n            mi = np.inner(c, x)\n            x_plus = x\n        if np.inner(c_tilde, x) < mi_tilde:\n            mi_tilde = np.inner(c_tilde, x)\n            x_minus = x\n        if mi + mi_tilde <= goal:\n            break\n    return x_plus, x_minus\n\n\ndef find_closest_cache(c):\n    m = inf\n    m_x = None\n    global previousPoints\n    for x in previousPoints.values():\n        if np.inner(c, x) < m:\n            m_x = x\n            m = np.inner(c, x)\n    return m_x\n\n\ndef reset_cache():\n    global previousPoints\n    # reset global statistic variables\n    previousPoints = {}\n\n\n####################################\n# for reporting on console\n####################################\ndef console_header(all_headers, run_config):\n    # under normal mode\n    header = all_headers[:3] + all_headers[4:6] + [all_headers[7]]\n    width = np.array([12, 8, 22, 22, 12, 12])\n    # it will be: ['Iteration', 'Type', 'Function Value', 'Dual Bound', '#Atoms', 'WTime']\n    if run_config['verbosity'] == 'verbose':\n        header += [all_headers[3]]  # add 'Primal Improve' to the console output\n        width = np.append(width, 22)\n    return header, width\n\n\ndef utils_console_info(all_info, run_config):\n    # under normal mode\n    info = all_info[:3] + all_info[4:6] + [all_info[7]]\n    if run_config['verbosity'] == 'verbose':\n        info += [all_info[3]]  # add 'Primal Improve' to the console output\n    return info\n\n","block_group":"179b370f9f904eddb3dc2fd939aa24e5","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1699362202976,"execution_millis":306,"deepnote_to_be_reexecuted":false,"cell_id":"15125602315241fb8b105caef7e3b08c","deepnote_cell_type":"code"},"source":"import numpy as np\nimport time\nimport logging\nimport os\nimport tableprint as tp\nfrom tabulate import tabulate\nimport signal\n\nclass GracefulKiller:\n    kill_now = False\n\n    def __init__(self):\n        signal.signal(signal.SIGINT, self.exit_gracefully)\n        signal.signal(signal.SIGTERM, self.exit_gracefully)\n\n    def exit_gracefully(self, signum, frame):\n        self.kill_now = True\n\n\nclass Algorithm:\n    \n    def __init__(self, feasible_region, objectiveFunction, run_config):\n        # parameter setting\n        self.feasible_region = feasible_region\n        self.objectiveFunction = objectiveFunction\n        self.run_config = run_config\n\n        self.continueRunning = True\n        self.enter_sub = True  # if enter the sub algorithm\n        self.func_val_all = np.array([])\n        self.dual_bound_all = np.array([])\n        self.wallClock_all = np.array([])\n        self.start_time = None\n\n        # for saving results\n        self.info_table = []\n        # there will be 10 columns of information in logging file\n        self.header = ['Iteration',\n                       'Type',\n                       'Function Value',\n                       'Primal Improve',\n                       'Dual Bound',\n                       '#Atoms',\n                       'Iteration Time',\n                       'WTime',\n                       'step size',\n                       'num_ls'\n                       ]\n        self.console_headerName, self.console_headerWidth = console_header(self.header, self.run_config)\n        self.it = 1  # main iteration counter\n        self.x_old = self.feasible_region.solve()  # returning initial vertex via LP call\n        if self.run_config['use_LPSep_oracle']:  # compute its initial bound\n            c = objectiveFunction.gradient(self.x_old)\n            initial_min_x = self.feasible_region.solve(c)\n            phi = np.inner(c, (self.x_old - initial_min_x)) / 2\n            self.dual_bound_all = np.append(self.dual_bound_all, phi)\n        self.s_list = [self.x_old.copy()]  # construct initial s_list\n        self.alpha = np.ones(1)  # weight vector\n\n        self.phi_recomputedIt = None\n\n    def update_return_info(self, val, phi):\n        self.func_val_all = np.append(self.func_val_all, val)\n        self.wallClock_all = np.append(self.wallClock_all, time.process_time() - self.start_time)\n        self.dual_bound_all = np.append(self.dual_bound_all, phi)\n\n    def find_index_s(self, s):\n        for i in range(len(self.s_list)):\n            if np.all(self.s_list[i] == s):\n                return i\n        return None\n\n    def fwStep_update_alpha_S(self, step_size, index_s, s):\n        if step_size == 1:\n            self.s_list = [s]  # set the s list to be this atom only\n            self.alpha = np.array([step_size])  # the weight for this atom is 1\n        else:\n            self.alpha = (1 - step_size) * self.alpha  # update weight for all the atoms originally in the s_list\n            if index_s is None:  # s is new\n                self.s_list.append(s)\n                self.alpha = np.append(self.alpha, step_size)\n            else:  # old atom\n                self.alpha[index_s] += step_size\n\n    def if_continue(self, phi, step_size, iter_type, killer):\n        if phi <= self.run_config['dual_gap_acc']:\n            logging.info('Exit Code 2: Achieved required dual gap accuracy, save results, and exit BCG algorithm.')\n            return False\n        # early stop for when reaching time limit\n        if self.run_config['runningTimeLimit'] is not None:\n            if self.wallClock_all[-1] > self.run_config['runningTimeLimit']:\n                logging.info('Exit Code 3: Reaching time limit, save current results, and exit BCG algorithm.')\n                return False\n        if killer.kill_now:\n            logging.error('Exit Code 0: Keyboard Interruption, save current results and exit BCG algorithm.')\n            return False\n        if step_size == 0 and iter_type != 'FI' and (self.it != self.phi_recomputedIt):\n            logging.error('Exit Code 1: No further primal progress, save current results, and exit BCG algorithm')\n            logging.error('Recomputing final dual gap.')\n            return False\n        return True\n\n    def recompute_phi(self):\n        est_phi = self.dual_bound_all[-1]  # estimated phi after current iteration\n        c = self.objectiveFunction.gradient(self.x_old)\n        s = self.feasible_region.solve(c)\n        actual_phi = np.inner(-c, s - self.x_old)  # actual phi after current iteration\n        phi = min(est_phi, actual_phi)\n        self.dual_bound_all[-1] = phi  # rewrite phi value for after current iteration\n        self.phi_recomputedIt = self.it\n        return phi\n\n    def run_algorithm(self):\n        global console_info\n        killer = GracefulKiller()\n        self.start_time = time.process_time()\n        with tp.TableContext(self.console_headerName, width=self.console_headerWidth) as t:\n            if self.run_config['verbosity'] == 'quiet':\n                logging.info('Above information is deactivated because of quiet running mode')\n            while self.continueRunning:\n                if self.enter_sub and self.it > 1:  # check whether to do SiGD iterations\n                    current_phi = self.dual_bound_all[-1]\n                    new_val = self.func_val_all[-1]\n\n                    grad_x = self.objectiveFunction.gradient(self.x_old)\n                    grad_alpha = np.dot(self.s_list, grad_x)\n                    min_grad_alpha = np.min(grad_alpha)\n                    max_grad_alpha = np.max(grad_alpha)\n\n                    k = 1  # iteration counter for sub iteration\n                    SIGD_improve = 'N/A'\n                    while self.continueRunning and max_grad_alpha - min_grad_alpha >= current_phi/self.run_config['K']:\n                        sigd_start_time = time.process_time()  # starting time of this iteration\n                        old_val = new_val.copy()  # function value from last iteration\n\n                        move_direction = -grad_alpha + np.sum(grad_alpha) / len(\n                            grad_alpha)  # decomposed negative gradient\n                        # compute the upper bound of step size, similar to simplex method\n                        d_neg_index = np.where(move_direction < 0)[0]\n                        tmp_neg = np.array([-self.alpha[j] / move_direction[j] for j in d_neg_index])\n                        eta_ub = np.min(tmp_neg)\n                        # line search along move direction with upper bound as eta_ub\n                        step_size, num_ls, SIGD_iteration_type = \\\n                            backtracking_ls_on_alpha(self.alpha, self.objectiveFunction,\n                                                           self.s_list, eta_ub,\n                                                           move_direction, self.run_config['max_lsSub'],\n                                                           SIGD_improve, self.run_config['strict_dropSteps'])\n\n                        if SIGD_iteration_type == 'PS' or k > self.run_config['max_stepsSub']:  # exit SIGD\n                            self.enter_sub = False  # will remain False until a new vertex is found\n                            if k > self.run_config['max_stepsSub']:\n                                SIGD_iteration_type = 'PS'\n                            sigd_end_time = time.process_time()  # ending time of this iteration\n                            self.update_return_info(old_val, current_phi)  # didn't move\n                            info = ['{}'.format(self.it), SIGD_iteration_type,\n                                    '{}'.format(old_val),  # didn't move, append old val\n                                    'N/A',  # here use 'N/A' instead of zero\n                                    '{}'.format(current_phi),\n                                    '{}'.format(len(self.s_list)),\n                                    '{:.4f}'.format(sigd_end_time - sigd_start_time),\n                                    '{:.4f}'.format(sigd_end_time - self.start_time),\n                                    '{}'.format(step_size),\n                                    '{}'.format(num_ls)\n                                    ]\n                            if not self.run_config['solution_only']:\n                                self.info_table.append(info)\n                            console_info = utils_console_info(info, self.run_config)\n                            if self.run_config['verbosity'] != 'quiet':\n                                t(console_info)\n                            self.it += 1\n                            break\n\n                        # update alpha and check if there is vertex that needs to be dropped\n                        self.alpha += step_size * move_direction\n                        if step_size == eta_ub:\n                            vertex_toBeDropped = np.argmin(tmp_neg)  # index in d_neg_index\n                            vertex_toBeDropped_index = d_neg_index[\n                                vertex_toBeDropped]  # retrieve index in the original alpha and s\n                            removeFromCache(self.s_list[vertex_toBeDropped_index])\n                            self.s_list.pop(vertex_toBeDropped_index)\n                            self.alpha = np.delete(self.alpha, vertex_toBeDropped_index)\n                            # normalize\n                            self.alpha *= 1 / np.sum(self.alpha)\n                            SIGD_iteration_type += 'D'\n\n                        # calculate new point and function value\n                        self.x_old = np.dot(np.transpose(self.s_list), self.alpha)\n                        new_val = self.objectiveFunction.evaluate(self.x_old)\n                        SIGD_improve = old_val - new_val\n                        grad_x = self.objectiveFunction.gradient(self.x_old)\n                        grad_alpha = np.dot(self.s_list, grad_x)\n                        min_grad_alpha = np.min(grad_alpha)\n                        max_grad_alpha = np.max(grad_alpha)\n                        sigd_end_time = time.process_time()  # ending time of this iteration\n                        self.update_return_info(new_val, current_phi)\n                        # save all the information\n                        info = ['{}'.format(self.it), SIGD_iteration_type,\n                                '{}'.format(new_val),\n                                SIGD_improve,  # corresponding to function value improve\n                                '{}'.format(current_phi),\n                                '{}'.format(len(self.s_list)),\n                                '{:.4f}'.format(sigd_end_time - sigd_start_time),\n                                '{:.4f}'.format(sigd_end_time - self.start_time),\n                                '{}'.format(step_size),\n                                '{}'.format(num_ls)\n                                ]\n                        if not self.run_config['solution_only']:\n                            self.info_table.append(info)\n                        console_info = utils_console_info(info, self.run_config)\n                        if self.run_config['verbosity'] != 'quiet':\n                            t(console_info)\n                        k += 1\n                        self.it += 1  # so we have all iterations counter\n                        self.continueRunning = self.if_continue(current_phi, step_size, SIGD_iteration_type, killer)\n                if not self.continueRunning:  # break from outside loop\n                    break\n\n                fw_start_time = time.process_time()  # starting time of FW iteration\n                # after sub, continue on FW or LCG step\n                c = self.objectiveFunction.gradient(self.x_old)\n                if self.run_config['use_LPSep_oracle']:  # call weak separation oracle\n                    s, iter_type = self.feasible_region.weak_sep(c, self.x_old, True, self.dual_bound_all[-1]/float(self.run_config['K']))\n                    direction = s - self.x_old\n                    if iter_type != 'FIC':  # if a new vertex is found, enable enter-sub\n                        self.enter_sub = True\n                    if iter_type == 'FN':  # if cannot find a better point\n                        direction = s - self.x_old\n                        est_phi = np.inner(-c, direction)\n                        if est_phi < 0:  # there is case that the LP solver stops before the optimal solution is found\n                            phi = self.dual_bound_all[-1]\n                        else:\n                            phi = min(self.dual_bound_all[-1]/2, est_phi / 2)\n                    else:  # found an improving vertex\n                        phi = self.dual_bound_all[-1]\n                else:  # FW\n                    s = self.feasible_region.solve(c)\n                    direction = s - self.x_old\n                    phi = np.inner(-c, direction)\n                    iter_type = 'F'\n\n                index_s = self.find_index_s(s)\n                if self.objectiveFunction.evaluate(s) <= self.objectiveFunction.evaluate(self.x_old) and \\\n                        len(self.s_list) >= 2*len(self.x_old):  # promote sparsity\n                    step_size, num_ls = 1, 'N/A'\n                else:\n                    step_size, num_ls = backtracking_ls_FW(self.objectiveFunction, self.x_old, c, direction,\n                                                                 self.run_config['max_lsFW'])\n                # update s and weight: fw update\n                self.fwStep_update_alpha_S(step_size, index_s, s)\n                # update x and function value\n                self.x_old += step_size * direction\n                new_val = self.objectiveFunction.evaluate(self.x_old)\n                if self.it == 1:\n                    func_val_improve = 'N/A'\n                else:\n                    func_val_improve = self.func_val_all[-1] - new_val\n                # only for reporting\n                if iter_type == 'FN':\n                    reporting_phi = self.dual_bound_all[-1]  # dual bound from last iteration\n                else:\n                    reporting_phi = phi\n                self.update_return_info(new_val, phi)  # save dual bound after current iteration\n                if step_size == 0 and iter_type != 'FI' and self.phi_recomputedIt is None:\n                    reporting_phi = self.recompute_phi()\n                info = ['{}'.format(self.it), iter_type,\n                        '{}'.format(new_val),\n                        '{}'.format(func_val_improve),\n                        '{}'.format(reporting_phi),\n                        '{}'.format(len(self.s_list)),\n                        '{:.4f}'.format(time.process_time() - fw_start_time),\n                        '{:.4f}'.format(time.process_time() - self.start_time),\n                        '{}'.format(step_size),\n                        num_ls\n                        ]\n                if not self.run_config['solution_only']:\n                    self.info_table.append(info)\n                console_info = utils_console_info(info, self.run_config)\n                if self.run_config['verbosity'] != 'quiet':\n                    t(console_info)\n                self.continueRunning = self.if_continue(phi, step_size, iter_type, killer)\n                self.it = self.it + 1\n\n        if not self.run_config['solution_only']:\n            with open(os.path.join(logging_dir, 'log.txt'), \"a\") as f:\n                print(tabulate(self.info_table, headers=self.header, tablefmt='presto'), file=f)\n\n        self.wallClock_all = self.wallClock_all - self.wallClock_all[0]\n        if self.run_config['use_LPSep_oracle']:\n            self.dual_bound_all = np.delete(self.dual_bound_all, 0)\n\n        return self.x_old, {\n            'func_val_all': self.func_val_all,\n            'wallClock_all': self.wallClock_all,\n            'dual_bound_all': self.dual_bound_all\n        }","block_group":"60f52b7dce9c485287118efde4ec0ac6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1699362202977,"execution_millis":273,"deepnote_to_be_reexecuted":false,"cell_id":"6aaeacfac7b24c619886bb5eae3c35f1","deepnote_cell_type":"code"},"source":"# contains oracles (LMO and Weak Separation)\n# contains feasible region construction (unit cube, L1Ball, Simple, or from lp file, Birkhoffpolytope, spectrahedron)\n\nimport abc\nimport numbers\nimport logging\nimport numpy as np\n\nclass Model(abc.ABC):\n    \"\"\"A generic class for an LP problem.\n\n    Attributes:\n\n    dimension: The number of coordinates feasible solutions have.\n\n\n    Implementation specific attributes:\n    These may be missing.\n\n    model: Dynamic data for optimizing over the model.\n    \"\"\"\n\n    def __init__(self, dimension):\n        \"\"\"Initialize a model.\"\"\"\n        assert (isinstance(dimension, numbers.Integral)\n                and dimension >= 0)\n        super().__init__()\n        self.dimension = dimension\n\n    @abc.abstractmethod\n    def minimize(self, cc=None):\n        \"\"\"Minimize objective cc.\"\"\"\n        pass\n\n    def augment(self, cc=None, x=None, goal=None):\n        # for models without usage of Gurobi: no early termination, return self.minimize(cc)\n        # for models with usage of Gurobi: this method will be redefined in LPsolver.py\n        \"\"\"Find a solution smaller than value goal for objective cc.\n        An already known solution is x if not None.\"\"\"\n        return self.minimize(cc)\n\n    def solve(self, cc=None):  # Linear Optimization Oracle\n        return self.minimize(cc)\n\n    def weak_sep(self, cc=None, x=None, strict=True, extra_margin=0):  # Weak Separation Oracle\n        \"\"\"Find a solution for cc with value smaller than that of x.\n            The value should be smaller by at least extra_margin.\"\"\"\n\n        if cc is not None and x is not None:\n            goal = np.inner(cc, x) - extra_margin\n            if strict:\n                goal -= accuracyComparison\n            else:\n                goal += accuracyComparison\n            if useCache:\n                y = checkForCache(cc, goal)\n                if y is not None:\n                    # logging.info('---> found cached point with <c, y> value {}'.format(np.inner(cc, y)))\n                    return y, 'FIC'\n        else:\n            logging.info('finding first feasible point ...... ')\n\n        s = self.augment(cc, x, goal)\n\n        if True:\n            if useCache:\n                addToCache(s)\n\n            # logging.info('condition: {}'.format(np.inner(cc, s) - goal))\n            if x is None or cc is None or (np.inner(cc, s) < goal):\n                # logging.info('found an improving vertex!')\n                return s, 'FI'\n            else:\n                # logging.info('Did not find better s!')\n                # logging.info('function value:{}'.format(np.inner(cc, s)))\n                # logging.info('old value:{}'.format(np.inner(cc, x)))\n                if useCache:  # add this point to cache\n                    s = find_closest_cache(cc)\n                return s, 'FN'","block_group":"192f6718db0e4c289692c1d15755804d","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1699362202977,"execution_millis":322,"deepnote_to_be_reexecuted":false,"cell_id":"60a16001da6a4dc4ba3d40ac3e3f689d","deepnote_cell_type":"code"},"source":"# conditional autograd import\nimport importlib.util\nspec = importlib.util.find_spec(\"autograd\")\nif spec is None:\n    import numpy as np\nelse:\n    import autograd.numpy as np\n    from autograd import grad\n\nimport logging\n\n#TODO: Logging shouldn't be configured when run as a module, it is the\n#task of the main application.\nlogging.basicConfig(level=logging.INFO,\n                    format='%(message)s')\n\n\ndef init_model(model):\n    if isinstance(model, str):  # LP file\n        from .LPsolver import initModel_fromFile\n        feasible_region = initModel_fromFile(model)\n    else:  # user-defined model class\n        feasible_region = model\n    return feasible_region\n\n\ndef BCG(f, f_grad, model, run_config=None):\n    # reset global statistic variables\n    reset_cache()\n\n    feasible_region = init_model(model)\n    dim = feasible_region.dimension\n\n    # autograd objective function is possible\n    if f_grad is None:\n        spec = importlib.util.find_spec(\"autograd\")\n        if spec is None or autograd is False:\n            objectiveFunction = ObjectiveFunction(f, f_grad)\n        else:\n            logging.info(\"Trying to use autograd to compute gradient...\")\n            try:\n                objectiveFunction = ObjectiveFunction(f, grad(f))\n            except:\n                logging.info(\"Autograd failed. Using numerical approximation.\")\n                objectiveFunction = ObjectiveFunction(f, f_grad)\n                pass\n    else:\n        objectiveFunction = ObjectiveFunction(f, f_grad)\n\n    logging.info('Dimension of feasible region: {}'.format(dim))\n\n    if f_grad is not None:  # check the user provided gradient oracle\n        logging.info('Checking validity of function gradient...')\n        sample_checkpoint = np.random.rand(dim)*10.0\n        check_res = objectiveFunction.check_gradient(sample_checkpoint)\n        assert check_res, 'gradient check error is {}'.format(check_res)\n\n    if run_config is None:  # make it global\n        run_config = run_config\n    # run algorithm\n    algorithmRun = BCGAlgorithm(feasible_region, objectiveFunction, run_config)\n    optimal_x, result_dict = algorithmRun.run_algorithm()\n    # dual_bound = result_dict['dual_bound_all'][-1]\n    # primal_val = result_dict['func_val_all'][-1]\n\n    # if not run_config['solution_only']:  # save the model pickle file in current working folder\n    #     import pickle\n    #     import os\n    #     pickle.dump(result_dict, open(os.getcwd() + '/model_result.p', 'wb'))\n    # return optimal_x, dual_bound, primal_val\n    return optimal_x\n","block_group":"c40b1454dd8f4aa29415f24450b03a2f","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1699362202978,"execution_millis":4354,"deepnote_to_be_reexecuted":false,"cell_id":"a946c0a914e74e55bffec924e3abd148","deepnote_cell_type":"code"},"source":"!pip install autograd\n","block_group":"29dc82ca8a594ef28cf1c481f91ca38a","execution_count":null,"outputs":[{"name":"stdout","text":"Collecting autograd\n  Downloading autograd-1.6.2-py3-none-any.whl (49 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: future>=0.15.2 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from autograd) (0.18.2)\nRequirement already satisfied: numpy>=1.12 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from autograd) (1.23.4)\nInstalling collected packages: autograd\nSuccessfully installed autograd-1.6.2\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1699362207330,"execution_millis":20,"deepnote_to_be_reexecuted":false,"cell_id":"c68e51c1708442d2afe914d4cdd8e388","deepnote_cell_type":"code"},"source":"# import autograd.numpy as np\n\n\n# from sklearn import datasets\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.model_selection import train_test_split\n\n# digits = datasets.load_digits()\n\n# A, y = digits.data, digits.target\n# A = StandardScaler().fit_transform(A)\n\n# # classify small against large digits\n# y = (y > 4).astype(np.int_)\n\n# A_train, A_test, y_train, y_test = train_test_split(\n#     A, y, train_size=1300, test_size=497, random_state=0)\n\n\n# # define a model class as feasible region\n# class Model_l1_ball(Model):\n#     def minimize(self, gradient_at_x=None):\n#         result = np.zeros(self.dimension)\n#         if gradient_at_x is None:\n#             result[0] = 1\n#         else:\n#             i = np.argmax(np.abs(gradient_at_x))\n#             result[i] = -1 if gradient_at_x[i] > 0 else 1\n#         return result\n\n\n# l1Ball = Model_l1_ball(A.shape[1])  # initialize the feasible region as a L1 ball\n\n\n# # you can construct your own configuration dictionary\n# config_dictionary = {\n#         'solution_only': False,\n#         'verbosity': 'verbose',\n#         'dual_gap_acc': 1e-06,\n#         'runningTimeLimit': 40,\n#         'use_LPSep_oracle': True,\n#         'max_lsFW': 30,\n#         'strict_dropSteps': True,\n#         'max_stepsSub': 1000,\n#         'max_lsSub': 30,\n#         'LPsolver_timelimit': 100,\n#         'K': 1\n#         }\n\n\n# scale_parameter = 5\n# # define function evaluation oracle\n# def f(x):\n#     return np.sum([np.log(np.exp(-y_train[i]*np.dot(scale_parameter*A_train[i], x))+1) for i in range(len(A_train))])\n\n\n# res = BCG(f, None, l1Ball, config_dictionary)\n# print('optimal solution {}'.format(res[0]))\n# print('dual_bound {}'.format(res[1]))\n\n\n# def predict(data, label, weight):\n#     correct = 0\n#     for i in range(len(data)):\n#         y_est = 1/(1+np.exp(-np.dot(scale_parameter*data[i], weight)))\n#         if y_est > 0.5:\n#             y_est = 1\n#         else:\n#             y_est = 0\n#         if y_est == label[i]:\n#             correct += 1\n#     return correct/len(data)\n\n\n# acc_train = predict(A_train, y_train, res[0])\n# print('accuracy on the training dataset {}'.format(acc_train))\n\n# acc_test = predict(A_test, y_test, res[0])\n# print('accuracy on the testing dataset {}'.format(acc_test))\n\n","block_group":"6c148b2f1a3c453cb64ca64783e82edb","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1699362207341,"execution_millis":70,"deepnote_to_be_reexecuted":false,"cell_id":"90a20501b96140ab92b767bce971fc14","deepnote_cell_type":"code"},"source":"class BCGAlgorithm:\n\n    def __init__(self, feasible_region, objectiveFunction, run_config):\n        # parameter setting\n        self.feasible_region = feasible_region\n        self.objectiveFunction = objectiveFunction\n        self.run_config = run_config\n\n        self.continueRunning = True\n        self.enter_sub = True  # if enter the sub algorithm\n        self.func_val_all = np.array([])\n        self.dual_bound_all = np.array([])\n        self.wallClock_all = np.array([])\n        self.start_time = None\n\n        # for saving results\n        self.info_table = []\n        # there will be 10 columns of information in logging file\n        self.header = ['Iteration',\n                       'Type',\n                       'Function Value',\n                       'Primal Improve',\n                    #    'Dual Bound',\n                       '#Atoms',\n                    #    'Iteration Time',\n                       'WTime',\n                       'step size',\n                       'num_ls'\n                       ]\n        self.console_headerName, self.console_headerWidth = console_header(self.header, self.run_config)\n        self.it = 1  # main iteration counter\n        self.x_old = self.feasible_region.solve()  # returning initial vertex via LP call\n        if self.run_config['use_LPSep_oracle']:  # compute its initial bound\n            c = objectiveFunction.gradient(self.x_old)\n            initial_min_x = self.feasible_region.solve(c)\n            phi = np.inner(c, (self.x_old - initial_min_x)) / 2\n            self.dual_bound_all = np.append(self.dual_bound_all, phi)\n        self.s_list = [self.x_old.copy()]  # construct initial s_list\n        self.alpha = np.ones(1)  # weight vector\n\n        self.phi_recomputedIt = None\n\n    def update_return_info(self, val, phi):\n        self.func_val_all = np.append(self.func_val_all, val)\n        self.wallClock_all = np.append(self.wallClock_all, time.process_time() - self.start_time)\n        self.dual_bound_all = np.append(self.dual_bound_all, phi)\n\n    def find_index_s(self, s):\n        for i in range(len(self.s_list)):\n            if np.all(self.s_list[i] == s):\n                return i\n        return None\n\n    def fwStep_update_alpha_S(self, step_size, index_s, s):\n        if step_size == 1:\n            self.s_list = [s]  # set the s list to be this atom only\n            self.alpha = np.array([step_size])  # the weight for this atom is 1\n        else:\n            self.alpha = (1 - step_size) * self.alpha  # update weight for all the atoms originally in the s_list\n            if index_s is None:  # s is new\n                self.s_list.append(s)\n                self.alpha = np.append(self.alpha, step_size)\n            else:  # old atom\n                self.alpha[index_s] += step_size\n\n    def if_continue(self, phi, step_size, iter_type, killer):\n        if phi <= self.run_config['dual_gap_acc']:\n            logging.info('Exit Code 2: Achieved required dual gap accuracy, save results, and exit BCG algorithm.')\n            return False\n        # early stop for when reaching time limit\n        if self.run_config['runningTimeLimit'] is not None:\n            if self.wallClock_all[-1] > self.run_config['runningTimeLimit']:\n                logging.info('Exit Code 3: Reaching time limit, save current results, and exit BCG algorithm.')\n                return False\n        if killer.kill_now:\n            logging.error('Exit Code 0: Keyboard Interruption, save current results and exit BCG algorithm.')\n            return False\n        if step_size == 0 and iter_type != 'FI' and (self.it != self.phi_recomputedIt):\n            logging.error('Exit Code 1: No further primal progress, save current results, and exit BCG algorithm')\n            logging.error('Recomputing final dual gap.')\n            return False\n        return True\n\n    def recompute_phi(self):\n        est_phi = self.dual_bound_all[-1]  # estimated phi after current iteration\n        c = self.objectiveFunction.gradient(self.x_old)\n        s = self.feasible_region.solve(c)\n        actual_phi = np.inner(-c, s - self.x_old)  # actual phi after current iteration\n        phi = min(est_phi, actual_phi)\n        self.dual_bound_all[-1] = phi  # rewrite phi value for after current iteration\n        self.phi_recomputedIt = self.it\n        return phi\n\n    def run_algorithm(self):\n        # global console_info\n        killer = GracefulKiller()\n        self.start_time = time.process_time()\n\n        with tp.TableContext(self.console_headerName, width=self.console_headerWidth) as t:\n            if self.run_config['verbosity'] == 'quiet':\n                logging.info('Above information is deactivated because of quiet running mode')\n\n            while self.it < 10:\n\n                grad_x = self.objectiveFunction.gradient(self.x_old)\n                grad_alpha = np.dot(self.s_list, grad_x)\n                \n                id_min = np.argmin(grad_alpha)\n                id_max = np.argmax(grad_alpha)\n\n                w_t = self.feasible_region.solve(grad_x)\n                \n                a = self.s_list[id_max]\n                s_t = self.s_list[id_min]\n                local_PW_gap = np.inner(grad_x, a - s_t)\n                FW_gap = np.inner(grad_x, self.x_old - w_t)\n\n                if local_PW_gap > FW_gap:\n\n                    d = a - s_t\n                    coeff_star = self.alpha[id_max]\n\n                    # line search along move direction with upper bound as eta_ub\n                    step_size, num_ls = backtracking_ls_delta(self.objectiveFunction, self.x_old, grad_x, -d, 10, coeff_star)\n\n                    # update alpha and check if there is vertex that needs to be dropped\n                    # print(\"sum alpha {}\".format(np.sum(self.alpha)))\n                    SIGD_iteration_type = 'Descent'\n                    if step_size == coeff_star:\n                        # self.alpha += step_size * d\n                        vertex_toBeDropped = a\n                        vertex_toBeDropped_index = id_max # retrieve index in the original alpha and s\n                        self.s_list.pop(vertex_toBeDropped_index)\n                        self.alpha = np.delete(self.alpha, vertex_toBeDropped_index)\n                        # normalize\n                        self.alpha *= 1 / np.sum(self.alpha)\n                        # print(\"step size 1 {}\".format(step_size))\n                        SIGD_iteration_type = 'Drop'\n\n                else:\n                    d = self.x_old - w_t\n                    step_size, num_ls = backtracking_ls_delta(self.objectiveFunction, self.x_old, grad_x, -d, 10, 1)\n                    # print(\"step size 2 {}\".format(step_size))\n                    # update s and weight: fw update                  \n                    self.fwStep_update_alpha_S(step_size, None, w_t)\n                    SIGD_iteration_type = 'FW'\n\n                self.x_old -= step_size * d\n                self.it = self.it + 1\n                sigd_end_time = time.process_time()\n                new_val = self.objectiveFunction.evaluate(self.x_old)\n                info = ['{}'.format(self.it), SIGD_iteration_type,\n                        '{}'.format(new_val),  # didn't move, append old val\n                        'N/A',  # here use 'N/A' instead of zero\n                        # '{}'.format(current_phi),\n                        '{}'.format(len(self.s_list)),\n                        # '{:.4f}'.format(sigd_end_time - sigd_start_time),\n                        '{:.4f}'.format(sigd_end_time - self.start_time),\n                        '{}'.format(step_size),\n                        '{}'.format(num_ls)\n                        ]\n                if not self.run_config['solution_only']:\n                    self.info_table.append(info)\n                console_info = utils_console_info(info, self.run_config)\n                if self.run_config['verbosity'] != 'quiet':\n                    t(console_info)\n\n        if not self.run_config['solution_only']:\n            with open(os.path.join(logging_dir, 'log.txt'), \"a\") as f:\n                print(tabulate(self.info_table, headers=self.header, tablefmt='presto'), file=f)\n\n        # self.wallClock_all = self.wallClock_all - self.wallClock_all[0]\n        # if self.run_config['use_LPSep_oracle']:\n        #     self.dual_bound_all = np.delete(self.dual_bound_all, 0)\n\n        return self.x_old, 1","block_group":"299afc5280c84e3994e557685bd79412","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1699362207454,"execution_millis":7209,"deepnote_to_be_reexecuted":false,"cell_id":"3219b3f5ee5c4b87ab5546a71fe5fa9b","deepnote_cell_type":"code"},"source":"from sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport autograd.numpy as np\n\ndigits = datasets.load_digits()\n\nA, y = digits.data, digits.target\nA = StandardScaler().fit_transform(A)\n\n# classify small against large digits\ny = (y > 4).astype(np.int_)\n\nA_train, A_test, y_train, y_test = train_test_split(\n    A, y, train_size=1300, test_size=497, random_state=0)\n\n\n# define a model class as feasible region\nclass Model_l1_ball(Model):\n    def minimize(self, gradient_at_x=None):\n        result = np.zeros(self.dimension)\n        if gradient_at_x is None:\n            result[0] = 1\n        else:\n            i = np.argmax(np.abs(gradient_at_x))\n            result[i] = -1 if gradient_at_x[i] > 0 else 1\n        return result\n\n\nl1Ball = Model_l1_ball(A.shape[1])  # initialize the feasible region as a L1 ball\n\n\n# you can construct your own configuration dictionary\nconfig_dictionary = {\n        'solution_only': False,\n        'verbosity': 'verbose',\n        'dual_gap_acc': 1e-06,\n        'runningTimeLimit': 40,\n        'use_LPSep_oracle': False,\n        'max_lsFW': 30,\n        'strict_dropSteps': True,\n        'max_stepsSub': 1000,\n        'max_lsSub': 30,\n        'LPsolver_timelimit': 100,\n        'K': 1\n        }\n\n\nscale_parameter = 5\n# define function evaluation oracle\ndef f(x):\n    return np.sum([np.log(np.exp(-y_train[i]*np.dot(scale_parameter*A_train[i], x))+1) for i in range(len(A_train))])\n\n\nres = BCG(f, None, l1Ball, config_dictionary)\nprint('optimal solution {}'.format(res))\n# print('dual_bound {}'.format(res[1]))\n\n\ndef predict(data, label, weight):\n    correct = 0\n    for i in range(len(data)):\n        y_est = 1/(1+np.exp(-np.dot(scale_parameter*data[i], weight)))\n        # print(y_est)\n        if y_est > 0.5:\n            y_est = 1\n        else:\n            y_est = 0\n        if y_est == label[i]:\n            correct += 1\n    return correct/len(data)\n\n\nacc_train = predict(A_train, y_train, res)\nprint('accuracy on the training dataset {}'.format(acc_train))\n\nacc_test = predict(A_test, y_test, res)\nprint('accuracy on the testing dataset {}'.format(acc_test))\n","block_group":"9d6d4c8986c64fc8bc09e8dabff92e2e","execution_count":null,"outputs":[{"name":"stderr","text":"Trying to use autograd to compute gradient...\nAutograd failed. Using numerical approximation.\nDimension of feasible region: 64\n╭──────────────┬──────────┬────────────────────────┬────────────────────────┬──────────────┬──────────────┬────────────────────────╮\n│    Iteration │     Type │         Function Value │                 #Atoms │        WTime │       num_ls │         Primal Improve │\n├──────────────┼──────────┼────────────────────────┼────────────────────────┼──────────────┼──────────────┼────────────────────────┤\n│            2 │       FW │       871.120122239295 │                      2 │       0.7293 │            2 │                    N/A │\n│            3 │       FW │      822.1863840365975 │                      3 │       1.4502 │            2 │                    N/A │\n│            4 │  Descent │      811.0875681777478 │                      3 │       2.1978 │            2 │                    N/A │\n│            5 │       FW │      791.2785994352005 │                      4 │       2.9694 │            2 │                    N/A │\n│            6 │       FW │      753.9204670015514 │                      5 │       3.6762 │            2 │                    N/A │\n│            7 │  Descent │      748.5781235806722 │                      5 │       4.3824 │            1 │                    N/A │\n│            8 │  Descent │      741.8777846265821 │                      5 │       5.0085 │            1 │                    N/A │\n│            9 │       FW │      725.2104148845467 │                      6 │       5.6530 │            3 │                    N/A │\n│           10 │       FW │      715.4336529697371 │                      7 │       6.3161 │            3 │                    N/A │\n╰──────────────┴──────────┴────────────────────────┴────────────────────────┴──────────────┴──────────────┴────────────────────────╯\noptimal solution [ 0.24224854  0.          0.          0.          0.          0.12112427\n  0.          0.          0.          0.          0.          0.\n  0.          0.          0.          0.          0.          0.\n  0.125       0.         -0.09570312  0.          0.          0.\n  0.          0.          0.          0.14355469  0.          0.\n  0.          0.          0.          0.          0.          0.109375\n  0.          0.          0.          0.          0.          0.\n  0.          0.          0.          0.          0.          0.\n  0.          0.          0.          0.         -0.16299438  0.\n  0.          0.          0.          0.          0.          0.\n  0.          0.          0.          0.        ]\naccuracy on the training dataset 0.8569230769230769\naccuracy on the testing dataset 0.8249496981891348\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1699362214649,"execution_millis":48,"deepnote_to_be_reexecuted":false,"cell_id":"d353ad327fd14c5e8a386b0330b4f0e5","deepnote_cell_type":"code"},"source":"","block_group":"345e9349ae3741389114a8d412d08a48","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=20d1bbe1-9699-47d1-9e58-55fbeb4c72ba' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iODBweCIgaGVpZ2h0PSI4MHB4IiB2aWV3Qm94PSIwIDAgODAgODAiIHZlcnNpb249IjEuMSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIiB4bWxuczp4bGluaz0iaHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluayI+CiAgICA8IS0tIEdlbmVyYXRvcjogU2tldGNoIDU0LjEgKDc2NDkwKSAtIGh0dHBzOi8vc2tldGNoYXBwLmNvbSAtLT4KICAgIDx0aXRsZT5Hcm91cCAzPC90aXRsZT4KICAgIDxkZXNjPkNyZWF0ZWQgd2l0aCBTa2V0Y2guPC9kZXNjPgogICAgPGcgaWQ9IkxhbmRpbmciIHN0cm9rZT0ibm9uZSIgc3Ryb2tlLXdpZHRoPSIxIiBmaWxsPSJub25lIiBmaWxsLXJ1bGU9ImV2ZW5vZGQiPgogICAgICAgIDxnIGlkPSJBcnRib2FyZCIgdHJhbnNmb3JtPSJ0cmFuc2xhdGUoLTEyMzUuMDAwMDAwLCAtNzkuMDAwMDAwKSI+CiAgICAgICAgICAgIDxnIGlkPSJHcm91cC0zIiB0cmFuc2Zvcm09InRyYW5zbGF0ZSgxMjM1LjAwMDAwMCwgNzkuMDAwMDAwKSI+CiAgICAgICAgICAgICAgICA8cG9seWdvbiBpZD0iUGF0aC0yMCIgZmlsbD0iIzAyNjVCNCIgcG9pbnRzPSIyLjM3NjIzNzYyIDgwIDM4LjA0NzY2NjcgODAgNTcuODIxNzgyMiA3My44MDU3NTkyIDU3LjgyMTc4MjIgMzIuNzU5MjczOSAzOS4xNDAyMjc4IDMxLjY4MzE2ODMiPjwvcG9seWdvbj4KICAgICAgICAgICAgICAgIDxwYXRoIGQ9Ik0zNS4wMDc3MTgsODAgQzQyLjkwNjIwMDcsNzYuNDU0OTM1OCA0Ny41NjQ5MTY3LDcxLjU0MjI2NzEgNDguOTgzODY2LDY1LjI2MTk5MzkgQzUxLjExMjI4OTksNTUuODQxNTg0MiA0MS42NzcxNzk1LDQ5LjIxMjIyODQgMjUuNjIzOTg0Niw0OS4yMTIyMjg0IEMyNS40ODQ5Mjg5LDQ5LjEyNjg0NDggMjkuODI2MTI5Niw0My4yODM4MjQ4IDM4LjY0NzU4NjksMzEuNjgzMTY4MyBMNzIuODcxMjg3MSwzMi41NTQ0MjUgTDY1LjI4MDk3Myw2Ny42NzYzNDIxIEw1MS4xMTIyODk5LDc3LjM3NjE0NCBMMzUuMDA3NzE4LDgwIFoiIGlkPSJQYXRoLTIyIiBmaWxsPSIjMDAyODY4Ij48L3BhdGg+CiAgICAgICAgICAgICAgICA8cGF0aCBkPSJNMCwzNy43MzA0NDA1IEwyNy4xMTQ1MzcsMC4yNTcxMTE0MzYgQzYyLjM3MTUxMjMsLTEuOTkwNzE3MDEgODAsMTAuNTAwMzkyNyA4MCwzNy43MzA0NDA1IEM4MCw2NC45NjA0ODgyIDY0Ljc3NjUwMzgsNzkuMDUwMzQxNCAzNC4zMjk1MTEzLDgwIEM0Ny4wNTUzNDg5LDc3LjU2NzA4MDggNTMuNDE4MjY3Nyw3MC4zMTM2MTAzIDUzLjQxODI2NzcsNTguMjM5NTg4NSBDNTMuNDE4MjY3Nyw0MC4xMjg1NTU3IDM2LjMwMzk1NDQsMzcuNzMwNDQwNSAyNS4yMjc0MTcsMzcuNzMwNDQwNSBDMTcuODQzMDU4NiwzNy43MzA0NDA1IDkuNDMzOTE5NjYsMzcuNzMwNDQwNSAwLDM3LjczMDQ0MDUgWiIgaWQ9IlBhdGgtMTkiIGZpbGw9IiMzNzkzRUYiPjwvcGF0aD4KICAgICAgICAgICAgPC9nPgogICAgICAgIDwvZz4KICAgIDwvZz4KPC9zdmc+' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"ae35e6d7fcf74a6eae85d742eb3cde68","deepnote_execution_queue":[]}}